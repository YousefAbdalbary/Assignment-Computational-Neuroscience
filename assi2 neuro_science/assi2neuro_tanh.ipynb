{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward phase \n",
      "prob of classs o1 : 0.5202420041482945\n",
      "prob of classs o2 : 0.5530399963719045\n",
      "It is class o2\n",
      "\n",
      "\n",
      "Weights:\n",
      "W1: -0.09603401288793112, W2: -0.43661594552011795, W3: -0.4031690191871149, W4: 0.07220138413761845\n",
      "W5: -0.18215527205766946, W6: -0.10234419772098569, W7: -0.3090283069087423, W8: 0.11843614595722673\n",
      "\n",
      "\n",
      "Backpropagation\n",
      "Updated Weights:\n",
      "W1: -0.09562342047402346, W2: -0.4357947606923026, W3: -0.4044378124693464, W4: 0.06966379757315545\n",
      "W5: -0.11009914167316495, W6: -0.025397634707936143, W7: -0.36562595120224495, W8: 0.05799723285183056\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def tanh(x):\n",
    "    exp_plus_x = 2.718281828459045 ** x\n",
    "    exp_neg_x = 2.718281828459045 ** -x\n",
    "    return (exp_plus_x - exp_neg_x) / (exp_plus_x + exp_neg_x)\n",
    "def random_weight():\n",
    "    return random.uniform(-.5,.5)\n",
    "def weighted_sum (inputs,weigths, bias):\n",
    "    return sum(i * w for i,w in zip(inputs,weigths))+bias\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1- x**2\n",
    "\n",
    "w1,w2,w3,w4 = random_weight(),random_weight(),random_weight(),random_weight()\n",
    "w5,w6,w7,w8 = random_weight(),random_weight(),random_weight(),random_weight()\n",
    "\n",
    "i1,i2 = .05,.1\n",
    "b1,b2 =.5 , .7\n",
    "learning_rate= 0.5  \n",
    "\n",
    "#hidden layer \n",
    "net_h1 = weighted_sum([i1,i2],[w1,w2],b1 ) \n",
    "out_h1 = tanh(net_h1)\n",
    "net_h2 = weighted_sum([i1,i2],[w3,w4],b1 ) \n",
    "out_h2 = tanh(net_h2)\n",
    "\n",
    "#output layer \n",
    "net_o1 = weighted_sum([out_h1,out_h2],[w5,w6],b2 ) \n",
    "out_o1 = tanh(net_o1)\n",
    "net_o2 = weighted_sum([out_h1,out_h2],[w7,w8],b2 ) \n",
    "out_o2 = tanh(net_o2)\n",
    "\n",
    "print(\"Forward phase \")\n",
    "print(f\"prob of classs o1 : {out_o1}\")\n",
    "print(f\"prob of classs o2 : {out_o2}\")\n",
    "print(\"It is class o1\") if (out_o1 > out_o2) else print(\"It is class o2\")\n",
    "print(\"\\n\\nWeights:\")\n",
    "print(f\"W1: {w1}, W2: {w2}, W3: {w3}, W4: {w4}\")\n",
    "print(f\"W5: {w5}, W6: {w6}, W7: {w7}, W8: {w8}\")\n",
    "actual_o1, actual_o2 = 0.01, 0.99 \n",
    "\n",
    "error_o1 = .5 *(out_o1 - actual_o1)**2\n",
    "error_o2 = .5 *(out_o2 - actual_o2)**2\n",
    "total_error =error_o1 + error_o2\n",
    "\n",
    "delta_o1 = -(actual_o1 - out_o1) * tanh_derivative(net_o1)\n",
    "delta_o2 = -(actual_o2 - out_o2) * tanh_derivative(net_o2)\n",
    "\n",
    "deltaH1 = (delta_o1 * w5 + delta_o2 * w7) * tanh_derivative(net_h1)\n",
    "deltaH2 = (delta_o1 * w6 + delta_o2 * w8) * tanh_derivative(net_h2)\n",
    "\n",
    "\n",
    "w1 += learning_rate * deltaH1 * i1\n",
    "w2 += learning_rate * deltaH1 * i2\n",
    "w3 += learning_rate * deltaH2 * i1\n",
    "w4 += learning_rate * deltaH2 * i2\n",
    "\n",
    "\n",
    "w5 += learning_rate * delta_o1 * out_h1\n",
    "w6 += learning_rate * delta_o1 * out_h2\n",
    "w7 += learning_rate * delta_o2 * out_h1\n",
    "w8 += learning_rate * delta_o2 * out_h2\n",
    "\n",
    "\n",
    "print('\\n\\nBackpropagation')\n",
    "print(\"Updated Weights:\")\n",
    "print(f\"W1: {w1}, W2: {w2}, W3: {w3}, W4: {w4}\")\n",
    "print(f\"W5: {w5}, W6: {w6}, W7: {w7}, W8: {w8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
