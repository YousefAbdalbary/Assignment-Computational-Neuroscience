{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward phase \n",
      "prob of classs o1 : 0.3592630803214539\n",
      "prob of classs o2 : 0.320325587584365\n",
      "It is class o1\n",
      "\n",
      "\n",
      "Weights:\n",
      "W1: 0.1096411542464183, W2: 0.2082889025385617, W3: -0.05505884474557188, W4: -0.08622874014454485\n",
      "W5: -0.4979301880200244, W6: -0.1846763121087711, W7: -0.3301016646000956, W8: -0.46057512167076486\n",
      "\n",
      "\n",
      "Backpropagation\n",
      "Updated Weights:\n",
      "W1: 0.10878483607173711, W2: 0.20657626618919933, W3: -0.059227744233626256, W4: -0.0945665391206536\n",
      "W5: -0.5702835140347664, W6: -0.25261662366070947, W7: -0.1863347285777451, W8: -0.3255768988905718\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def tanh(x):\n",
    "    exp_plus_x = 2.718281828459045 ** x\n",
    "    exp_neg_x = 2.718281828459045 ** -x\n",
    "    return (exp_plus_x - exp_neg_x) / (exp_plus_x + exp_neg_x)\n",
    "def random_weight():\n",
    "    return random.uniform(-.5,.5)\n",
    "def weighted_sum (inputs,weigths, bias):\n",
    "    return sum(i * w for i,w in zip(inputs,weigths))+bias\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1- x**2\n",
    "\n",
    "w1,w2,w3,w4 = random_weight(),random_weight(),random_weight(),random_weight()\n",
    "w5,w6,w7,w8 = random_weight(),random_weight(),random_weight(),random_weight()\n",
    "\n",
    "i1,i2 = .05,.1\n",
    "b1,b2 =.5 , .7\n",
    "learning_rate= 0.5  \n",
    "\n",
    "#hidden layer \n",
    "net_h1 = weighted_sum([i1,i2],[w1,w2],b1 ) \n",
    "out_h1 = tanh(net_h1)\n",
    "net_h2 = weighted_sum([i1,i2],[w3,w4],b1 ) \n",
    "out_h2 = tanh(net_h2)\n",
    "\n",
    "#output layer \n",
    "net_o1 = weighted_sum([out_h1,out_h2],[w5,w6],b2 ) \n",
    "out_o1 = tanh(net_o1)\n",
    "net_o2 = weighted_sum([out_h1,out_h2],[w7,w8],b2 ) \n",
    "out_o2 = tanh(net_o2)\n",
    "\n",
    "print(\"Forward phase \")\n",
    "print(f\"prob of classs o1 : {out_o1}\")\n",
    "print(f\"prob of classs o2 : {out_o2}\")\n",
    "print(\"It is class o1\") if (out_o1 > out_o2) else print(\"It is class o2\")\n",
    "print(\"\\n\\nWeights:\")\n",
    "print(f\"W1: {w1}, W2: {w2}, W3: {w3}, W4: {w4}\")\n",
    "print(f\"W5: {w5}, W6: {w6}, W7: {w7}, W8: {w8}\")\n",
    "actual_o1, actual_o2 = 0.01, 0.99 \n",
    "\n",
    "error_o1 = .5 *(out_o1 - actual_o1)**2\n",
    "error_o2 = .5 *(out_o2 - actual_o2)**2\n",
    "total_error =error_o1 + error_o2\n",
    "\n",
    "delta_o1 = -(actual_o1 - out_o1) * tanh_derivative(net_o1)\n",
    "delta_o2 = -(actual_o2 - out_o2) * tanh_derivative(net_o2)\n",
    "\n",
    "deltaH1 = (delta_o1 * w5 + delta_o2 * w7) * tanh_derivative(net_h1)\n",
    "deltaH2 = (delta_o1 * w6 + delta_o2 * w8) * tanh_derivative(net_h2)\n",
    "\n",
    "\n",
    "w1 -= learning_rate * deltaH1 * i1\n",
    "w2 -= learning_rate * deltaH1 * i2\n",
    "w3 -= learning_rate * deltaH2 * i1\n",
    "w4 -= learning_rate * deltaH2 * i2\n",
    "\n",
    "\n",
    "w5 -= learning_rate * delta_o1 * out_h1\n",
    "w6 -= learning_rate * delta_o1 * out_h2\n",
    "w7 -= learning_rate * delta_o2 * out_h1\n",
    "w8 -= learning_rate * delta_o2 * out_h2\n",
    "\n",
    "\n",
    "print('\\n\\nBackpropagation')\n",
    "print(\"Updated Weights:\")\n",
    "print(f\"W1: {w1}, W2: {w2}, W3: {w3}, W4: {w4}\")\n",
    "print(f\"W5: {w5}, W6: {w6}, W7: {w7}, W8: {w8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
